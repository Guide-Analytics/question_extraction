{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import spacy\n",
    "import textacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def isNegative(token,sentence):\n",
    "    \"\"\"\n",
    "    token,tokens.doc -> bool\n",
    "    Takes a token representing a word and a doc representing a sentence\n",
    "    Returns whether the word is negated in the sentence\n",
    "    \"\"\"\n",
    "    for word in sentence:\n",
    "        if word.dep_ == 'neg': #if word is a negation\n",
    "            if word.head==token: #check if it negates the desired word\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def inClause(token,sentence):\n",
    "    \"\"\"\n",
    "    token,tokens.doc -> bool\n",
    "    Takes a token representing a word and a doc representing a sentence\n",
    "    Returns whether the word is part of a subordinate clause rather than the main clause\n",
    "    However, clauses subordinated by certain verbs of knowing or asking are included with main clauses\n",
    "    \"\"\"\n",
    "    clausetypes = ['advcl','relcl','csubj','csubjpass','pcomp','xcomp','acl','aux']\n",
    "    knowing = ['know','understand','see','get']\n",
    "    asking = ['wonder','ask','inquire','demand']\n",
    "    while token.dep_ != 'ROOT':\n",
    "        if token.dep_ in clausetypes:\n",
    "            return True\n",
    "        if token.dep_ == 'ccomp' or token.dep_ == 'conj':\n",
    "            if token.head.lemma_ in knowing: #if it is a verb of knowing, only use negatives\n",
    "                return not isNegative(token.head,sentence)\n",
    "            if token.head.lemma_ in asking:\n",
    "                return False\n",
    "            return True\n",
    "        token=token.head\n",
    "    return token.pos_ != 'VERB' and token.pos_ != 'AUX' #phrase head should be a verb\n",
    "\n",
    "def isSubject(token):\n",
    "    \"\"\"\n",
    "    token -> bool\n",
    "    Takes a token representing a word\n",
    "    Returns whether the word is part of the noun phrase representing the subject of a sentence\n",
    "    \"\"\"\n",
    "    while token.dep_ != 'ROOT': #iterate until you get to the main verb\n",
    "        if token.dep_ == 'nsubj': #if you get to the head of the subject noun phrase first, then the original word was part of this phrase\n",
    "            return True\n",
    "        token = token.head\n",
    "    return False\n",
    "\n",
    "def inPhrase(token1, token2):\n",
    "    \"\"\"\n",
    "    token, token ->\n",
    "    \"\"\"\n",
    "    while token1.dep_ != 'ROOT':\n",
    "        if token1==token2:\n",
    "            return True\n",
    "        token1=token1.head\n",
    "    if token1==token2:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def hasAux(token, sentence):\n",
    "    \"\"\"\n",
    "    token, tokens.doc -> bool\n",
    "    Takes a word and a sentences\n",
    "    Returns whether the word is modified by an auxiliary in the sentence (or is one)\n",
    "    \"\"\"\n",
    "    if token.pos_ == 'AUX':\n",
    "        return True\n",
    "    for word in sentence:\n",
    "        if inPhrase(word,token):\n",
    "            temp = word\n",
    "            while temp.dep_ != 'ROOT':\n",
    "                if temp.pos_ == 'AUX':\n",
    "                    return True\n",
    "                temp = temp.head\n",
    "    return False\n",
    "\n",
    "def isYNQuestion(sentence):\n",
    "    \"\"\"\n",
    "    tokens.doc -> bool\n",
    "    Takes a string representing a sentence\n",
    "    Returns whether the sentence is a yes or no question\n",
    "    \"\"\"\n",
    "    if sentence[0].pos_ == 'AUX': #yn questions start with an auxiliary verb\n",
    "        if isSubject(sentence[1]): #it is followed by the subject\n",
    "            if sentence[0].dep_=='aux':\n",
    "                return sentence[0].head.dep_ == 'ROOT' #eliminates some rare adverbial clauses\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isWHQuestion(sentence):\n",
    "    \"\"\"\n",
    "    tokens.doc -> bool\n",
    "    Takes a string representing a sentence\n",
    "    Returns whether the sentence is a wh-question, i.e. who-what-where-when-why\n",
    "    \"\"\"\n",
    "    whwords = ['who', 'what', 'where', 'when', 'why', 'how', 'which', 'whose', 'whence', 'whither', 'whom']\n",
    "    for word in sentence:\n",
    "        if word.lemma_ in whwords: #for each wh-word, see if it is in the main clause\n",
    "            if inClause(word,sentence):\n",
    "                continue #if not, continue looking for wh-words\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isQuestion(sentence):\n",
    "    \"\"\"\n",
    "    str -> bool\n",
    "    Takes a string representing a sentence\n",
    "    Returns whether the sentence is a question\n",
    "    \"\"\"\n",
    "    text = nlp(sentence) #split up sentence into words\n",
    "    if len(text)<3:\n",
    "        return False\n",
    "    return isYNQuestion(text) or isWHQuestion(text)\n",
    "\n",
    "def extractQuestions(text):\n",
    "    \"\"\"\n",
    "    .csv -> list\n",
    "    Takes a csv file with some sentences\n",
    "    Returns the subset of sentences that are questions as a list\n",
    "    \"\"\"\n",
    "    sentences = list(text)[0] #convert csv into list\n",
    "    questions = [] #output list of questions\n",
    "    for element in sentences:\n",
    "        if isQuestion(element): #check if the sentence is a question\n",
    "            questions.append(element) #if so, add it to the list\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from cleantext import clean\n",
    "\n",
    "def extractQuestionsStr(sentences):\n",
    "    \"\"\"\n",
    "    list -> list\n",
    "    Takes list with strings representing sentences\n",
    "    Returns the subset of sentences that are questions as a list\n",
    "    \"\"\"\n",
    "    questions = [] #output list of questions\n",
    "    for element in sentences:\n",
    "        if isQuestion(element): #check if the sentence is a question\n",
    "            questions.append(element) #if so, add it to the list\n",
    "    return questions\n",
    "\n",
    "raw_data = pd.read_csv(\"GOOGLE_REVIEWS_Walmart_Guelph_1_2019-09-30.csv\")\n",
    "raw_data = raw_data.dropna(subset=['Review Text'])\n",
    "raw_data = raw_data.reset_index(drop=True)\n",
    "reviews = raw_data['Review Text']\n",
    "with open('output.csv',mode='w') as out_file:\n",
    "    writer = csv.writer(out_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for i in range(len(reviews)):\n",
    "        raw_sentences = re.split('[ ]*[.?!;\\n]+[ \\n]*',reviews[i]) #split text up into sentences, remove whitespace\n",
    "        cleaned_sentences = map(lambda s: clean(s.encode('ascii',errors='ignore').decode()), raw_sentences) #remove non-ascii characters\n",
    "        questions = extractQuestionsStr(cleaned_sentences)\n",
    "        print(reviews[i])\n",
    "        print(questions)\n",
    "        if len(questions)>0:\n",
    "            writer.writerow(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PRP$ PRON wife poss\n",
      "wife NN NOUN picking nsubj\n",
      "was VBD AUX picking aux\n",
      "picking VBG VERB picking ROOT\n",
      "up RP ADP picking prt\n",
      "some DT DET salt det\n",
      "water NN NOUN softener compound\n",
      "softener NN NOUN salt compound\n",
      "salt NN NOUN picking dobj\n",
      "and CC CCONJ picking cc\n",
      "asked VBD VERB picking conj\n",
      "for IN ADP asked prep\n",
      "assistance NN NOUN for pobj\n",
      "loading VBG VERB assistance acl\n",
      "it PRP PRON loading dobj\n",
      "into IN ADP loading prep\n",
      "the DT DET car det\n",
      "car NN NOUN into pobj\n",
      "and CC CCONJ picking cc\n",
      "was VBD AUX told auxpass\n",
      "told VBN VERB picking conj\n",
      "by IN ADP told agent\n",
      "the DT DET cashier det\n",
      "cashier NN NOUN by pobj\n",
      "that IN SCONJ know mark\n",
      "she PRP PRON know nsubj\n",
      "did VBD AUX know aux\n",
      "n't RB PART know neg\n",
      "know VB VERB told ccomp\n",
      "who WP PRON help nsubj\n",
      "could MD AUX help aux\n",
      "help VB VERB help ROOT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(token):\n",
    "    print(token.text)\n",
    "    temp = token\n",
    "    temp = temp.head\n",
    "    print(token.text)\n",
    "\n",
    "text = nlp(\"My wife was picking up some water softener salt and asked for assistance loading it into the car and was told by the cashier that she didn't know who could help\")\n",
    "for token in text:\n",
    "    print(token.text, token.tag_, token.pos_, token.head.text, token.dep_)\n",
    "isQuestion(\"My wife was picking up some water softener salt and asked for assistance loading it into the car and was told by the cashier that she didn't know who could help\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\") as f_obj:\n",
    "    raw_text = f_obj.read()\n",
    "    raw_sentences = re.split('[ ]*[.?!;\\n]+[ \\n]*',raw_text)\n",
    "    cleaned_sentences = map(lambda s: s.encode('ascii',errors='ignore').decode(), raw_sentences)\n",
    "    questions = extractQuestionsStr(cleaned_sentences)\n",
    "    print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
